source('C:/Users/Pichau/Desktop/TwitterBot/funcs_blog.R', echo=TRUE)
isntall.packages("xml2")
install.packages("xml2")
install.packages("xml2", dependencies = TRUE)
checkBlog()
dife <- setdiff(newList,oldList)
pag <- read_html(url)
library(xml2)
parse_website <- function(url){
require(xml2)
pag <- read_html(url)
fs <- xml_find_all(pag,xpath = "//ul[@id= 'post-list']/li/a")
ss <- xml_attr(fs,"href")
return(ss)
}
checkBlog <- function(newList,oldList){
dife <- setdiff(newList,oldList)
teste <- length(dife)
if(teste==0){
print("No updates")
} else{
readline(prompt = "New post. Press [enter] to continue")
new_post_url <- dife[1]
new_post_url <- new_post_url
return(new_post_url)
}
}
checkBlog()
library(dplyr)
library(xml2)
library(telegram.bot)
rm(list=ls())
parse_website <- function(url){
require(xml2)
pag <- read_html(url)
fs <- xml_find_all(pag,xpath = "//h3[@class= 'item-title']/a")
ss <- xml_attr(fs,"href")
return(ss)
}
checkBlog <- function(newList,oldList,token){
require(Rfacebook)
teste <- prod(newList == oldList)
if(teste==1){
return("No updates")
} else{
readline(prompt= "New post. Press [enter] to continue")
new_post_url <- newList[newList != oldList][1]
new_post_url <- paste0("https://azul.netlify.com/",new_post_url)
updateStatus("Novo post", token = token, link = new_post_url)
}
}
save(obj, file = "Face_update/Lista.Rdata")
load(paste0(getwd(),"/Face_update/dados.Rdata"))
load(paste0(getwd(),"/Face_update/Lista.Rdata"))
site <- parse_website("https://azul.netlify.com/")
post <- checkBlog(site,obj)
bot <- Bot(token = bot_token('azulblog_bot'))
updates <- bot$getUpdates()
parse_website <- function(url){ #só lê o blog e busca por posts
require(xml2)
pag <- read_html(url)
fs <- xml_find_all(pag,xpath = "//ul[@id= 'post-list']/li/a")
ss <- xml_attr(fs,"href")
return(ss)
}
source('C:/Users/Pichau/Desktop/TwitterBot/bot_coutinho.R', encoding = 'UTF-8', echo=TRUE)
library(dplyr)
library(xml2)
library(telegram.bot)
rm(list=ls())
parse_website <- function(url){ #só lê o blog e busca por posts
require(xml2)
pag <- read_html(url)
fs <- xml_find_all(pag,xpath = "//ul[@id= 'post-list']/li/a")
ss <- xml_attr(fs,"href")
return(ss)
}
parse_website("https://azul.netlify.com/")
checkBlog <- function(newList,oldList){ #Lido o blog, você pode comparar uma lista com os posts "antigos" com os posts novos e se tiver um post novo ele retorna o link pro post novo. A lista não precisa ser um objeto lista, pode ser um vetor com as urls que o parse_website cospe
dife <- setdiff(newList,oldList)
teste <- length(dife)
if(teste==0){
print("No updates")
} else{
readline(prompt = "New post. Press [enter] to continue")
new_post_url <- dife[1]
new_post_url <- new_post_url
return(new_post_url)
}
}
checkBlog()
parse_website("https://azul.netlify.com/")
nova = parse_website(site)
site = "https://azul.netlify.com/"
rm(list=ls())
site = "https://azul.netlify.com/"
parse_website <- function(url){ #só lê o blog e busca por posts
require(xml2)
pag <- read_html(url)
fs <- xml_find_all(pag,xpath = "//ul[@id= 'post-list']/li/a")
ss <- xml_attr(fs,"href")
return(ss)
}
checkBlog <- function(newList,oldList){ #Lido o blog, você pode comparar uma lista com os posts "antigos" com os posts novos e se tiver um post novo ele retorna o link pro post novo. A lista não precisa ser um objeto lista, pode ser um vetor com as urls que o parse_website cospe
dife <- setdiff(newList,oldList)
teste <- length(dife)
if(teste==0){
print("No updates")
} else{
readline(prompt = "New post. Press [enter] to continue")
new_post_url <- dife[1]
new_post_url <- new_post_url
return(new_post_url)
}
}
nova = parse_website(site)
velha = parse_website(site)[-1]
velha
nova
checkBlog(nova,
velha)
checkBlog <- function(newList,oldList){ #Lido o blog, você pode comparar uma lista com os posts "antigos" com os posts novos e se tiver um post novo ele retorna o link pro post novo. A lista não precisa ser um objeto lista, pode ser um vetor com as urls que o parse_website cospe
dife <- setdiff(newList,oldList)
teste <- length(dife)
if(teste==0){
print("No updates")
} else{
new_post_url <- dife[1]
new_post_url <- new_post_url
return(new_post_url)
}
}
nova = parse_website(site)
velha = parse_website(site)[-1]
checkBlog(nova,
velha)
tweet = paste("Post novo: ",
post, sep = "")
post = checkBlog(nova,
velha)
tweet = paste("Post novo: ",
post, sep = "")
tweet
source('C:/Users/Pichau/Desktop/TwitterBot/bot.R', encoding = 'UTF-8', echo=TRUE)
library(dplyr)
library(xml2)
site = "https://azul.netlify.com/"
parse_website <- function(url){ #só lê o blog e busca por posts
require(xml2)
pag <- read_html(url)
fs <- xml_find_all(pag,xpath = "//ul[@id= 'post-list']/li/a")
ss <- xml_attr(fs,"href")
return(ss)
}
checkBlog <- function(newList,oldList){ #Lido o blog, você pode comparar uma lista com os posts "antigos" com os posts novos e se tiver um post novo ele retorna o link pro post novo. A lista não precisa ser um objeto lista, pode ser um vetor com as urls que o parse_website cospe
dife <- setdiff(newList,oldList)
teste <- length(dife)
if(teste==0){
print("No updates")
} else{
new_post_url <- dife[1]
new_post_url <- new_post_url
return(new_post_url)
}
}
nova = parse_website(site)
velha = parse_website(site)[-1]
post = checkBlog(nova,
velha)
tweet = paste("Post novo: ",
post, sep = "")
tweet
## chaves específicas do bot da Azul, não vai funcionar em outro bot
consumerKey = 'DI7LdAavAOI0ECX6Q9UJaut36'
consumerSecret = 'JG9b7yXIRzVGqUNdnnYyj7aN8V2rKScg0Ki3FInmYejg5FOU7b'
accessToken = '1095031271540047873-WR3WQ6Uxhz17dltsEcULPijBvfNwjA'
accessTokenSecret = 'itq8Mr9Bu1l4NUtmtFrLF0zmhyuvp0Plvg85OidmvBN7z'
twitteR::setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
tweet %>% twitteR::tweet()
parse_website(site)
post
parse_website(post)
